%\documentclass{beamer}
\documentclass[handout]{beamer}
%\usepackage[dvips]{color}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{../macros}
\usepackage{verbatim}

\usetheme{default}
\title{Identifiability, Gauss Markov \& Predictive Distributions}
\subtitle{Merlise Clyde}
\author{STA721 Linear Models}
\institute{Duke University}
\date{\today}
\logo{duke.eps}

\begin{document}
\maketitle

\begin{frame}\frametitle{Outline}
Topics
  \begin{itemize}
  \item Gauss-Markov Theorem
  \item Estimability and Prediction
  \end{itemize}


Readings: Christensen Chapter 2,  Chapter 6.3, ( Appendix A, and
Appendix B as needed)
\end{frame}

\begin{frame}
\frametitle{Non-Identifiable }
  Recall the One-way ANOVA model
$$\mu_{ij} = \mu + \tau_j \qquad \mub = (
    \mu_{11}, \ldots,\mu_{n_1 1},\mu_{12},\ldots, \mu_{n_2,2},\ldots, \mu_{1J},
\ldots,
\mu_{n_J J})^T $$
\begin{itemize}
\item Let $\b_{1} = (\mu, \tau_1, \ldots, \tau_J)^T$
\item Let $\b_{2} = (\mu - 42, \tau_1 + 42, \ldots, \tau_J + 42)^T$
\item Then $\mub_{1} = \mub_{2}$ even though  $\b_1 \neq \b_2$
\item $\b$ is not identifiable
\item yet  $\mub$ is identifiable, where $\mub = \X \b$  (a linear
  combination of $\b$)
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Identifiability and Estimability}
  \begin{theorem}
    A function $g(\b)$ is identifiable if and only if $g(\b)$ is a
    function of $\mu(\b)$
  \end{theorem} \pause
In linear models, focus on linear functions.  Identifiable linear
functions are called {\it estimable} functions historically \pause

\begin{definition}
  A scalar function $\lambdab^T \b$ is {\it estimable} if $\lambdab^T \b
  = \a^T \X \b$ for some vector $\a \in \bbR^n$ \pause
\end{definition}
Equivalently

\begin{definition}
  A  function $\lambdab^T \b$ is {\it estimable} if it has
  an unbiased linear estimator,   i.e. there exists an $\a$ such that
  $\E(\a^T \Y) = \lambdab^T \b$ for all $\b$
\end{definition}

\end{frame}
\begin{frame}
  \frametitle{Estimability}
  \begin{theorem}
    The function $\psi = \lambdab^T \beta$ is estimable if and only if
    $\lambdab^T$ is a linear combination of the rows of
    $\X$. i.e. there exists $\a^T$ such that  $\lambdab^T = \a^T \X$
  \end{theorem} \pause
  \begin{proof}
   The function $\psi= \lambdab^T\b$ is estimable if there exists an $\a^T$ such that
    $\E[\a^T\Y] = \lambdab^T\b$ \pause

    \begin{eqnarray*}
      \E[\a^T\Y] & =  &\a^T \E[\Y] \pause\\
                 & = & \a^T \X \b \pause \\
  & =& \lambdab^T \b  \pause
    \end{eqnarray*}
if and only if $\lambdab^T = \a^T \X$ for all $\b$
  \end{proof}
\end{frame}
\begin{frame}
  \frametitle{Estimability of Individual $\beta_j$}
  \begin{block}{Proposition}
For    $$\mub = \X \b = \sum_j \X_j \beta_j$$
$\beta_j$ is not identifiable  if and only if there exists $\alpha_j$
such that $\X_j = \sum_{i \neq j} \X_i \alpha_i$
  \end{block} \pause
One-way Anova Model:
$$Y_{ij} = \mu + \tau_j + \epsilon_{ij}$$
  $$ \mub =  \left[
    \begin{array}{lllll}
\one_{n_1} & \one_{n_1} & \zero_{n_1} &  \ldots & \zero_{n_1} \\
\one_{n_2} & \zero_{n_2} & \one_{n_2} &  \ldots & \zero_{n_2} \\
\vdots & \vdots & \ddots & \vdots \\
\one_{n_J} & \zero_{n_J} & \zero_{n_J} &  \ldots & \one_{n_J} \\
    \end{array} \right]
 \left(   \begin{array}{l}
      \mu \\
      \tau_1 \\
   \tau_2 \\
 \vdots \\
\tau_J
    \end{array} \right)
$$\pause
Are any parameters $\mu$ or $\tau_j$ identifiable?
\end{frame}
\begin{frame}
  \frametitle{Gauss-Markov Theorem}
  \begin{theorem}

  Under the assumptions:
  \begin{eqnarray*}
    \E[\Y] & = & \mub \pause \\
    \Cov(\Y) & = &\sigma^2 \I_n \pause
  \end{eqnarray*}
every estimable function $\psi = \lambdab^T\b$ has a unique unbiased
linear estimator $\hat{\psi}$ which has minimum variance in the class
of all unbiased linear estimators. \pause  $\hat{\psi} = \lambdab^T\bhat$
where $\bhat$ is any set of ordinary least squares
estimators.

  \end{theorem}
\end{frame}
\begin{frame}
  \frametitle{Unique Unbiased Estimator}
  \begin{block}{Lemma}
    \begin{itemize}
    \item
    If   $\psi = \lambdab^T \b$ is estimable, there exists a unique
    linear unbiased estimator of $\psi = \a^{*T}\Y$ with $a^* \in
    C(\X)$. \pause
\item If $\a^T\Y$ is any unbiased linear estimator of $\psi$
    then $a^*$ is the projection of $\a$ onto $C(\X)$, i.e. $\a^* =
    \P_\X \a$.
    \end{itemize}

  \end{block}
      \end{frame}
  \begin{frame}
 \frametitle{Unique Unbiased Estimator}
  \begin{block}{Proof}
    \begin{itemize}
    \item     Since $\psi$ is estimable, there exists an $\a \in \bbR^n$ for
    which $\E[\a^T\Y] = \lambdab^T\b = \psi$  with $\lambdab^T =
    \a^T\X$ \pause
\item Let $\a = \a^* + \u$ where $\a^* \in C(\X)$ and $\u \in
  C(\X)^\perp$ \pause
\item Then
  \begin{eqnarray*}
\psi  =   \E[\a^T\Y] &=& \E[\a^{*T}\Y] + \alert<3-5>{\E[\u^T\Y]} \pause\\
     & = & \E[\a^{*T} \Y] + \alert<4>{0} \\
  &  & \\
\E[\u^T\Y] & = & \u^T\X \b
  \end{eqnarray*}
since $\u \perp C(\X)$ (i.e. $\u \in C(\X)^\perp$) $\E[\u^T\Y] = 0$ \pause

\item Thus $\a^{*T} \Y$ is also an unbiased linear estimator of $\psi$ with
  $\a^* \in C(\X)$
    \end{itemize}

  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Uniqueness}
  \begin{proof}
  Suppose that there is another $\v \in C(\X)$ such that $\E[\v^T\Y] =
  \psi$. Then for all $\b$ \pause
  \begin{eqnarray*}
    0 & = & \E[\a^{*T}\Y] - \E[\v^T\Y]  \pause \\
      & = & (\a^* - \v)^T \X \b \pause\\
\text{So  } (\a^* - \v)^T\X & = &  0 \quad \text{ for all } \b \pause
  \end{eqnarray*}

\begin{itemize}
\item Implies $(\a^* - \v) \in C(\X)^\perp$ \pause
\item but by assumption $(\a^* - \v) \in C(\X)$ \pause ($C(\X)$ is a
  vector space) \pause
\item the only vector in BOTH is $\zero$, so $\a^* = \v$ \pause
\end{itemize}
Therefore $\a^{*T}\Y$ is the unique linear unbiased estimator of $\psi$
with $\a^* \in C(\X)$.
  \end{proof}

\end{frame}
\begin{frame} \frametitle{Proof of Minimum Variance (G-M)}
 \begin{block}{}
   \begin{itemize}
    \item
    Let $\a^{*T}\Y$ be the unique unbiased linear estimator of $\psi$
    with $\a^* \in C(\X)$. \pause
\item Let $\a^T\Y$ be any unbiased estimate of $\psi$; $\a = \a^* +
  \u$ with $\a^* \in C(\X)$ and $\u \in C(\X)^\perp$ \pause
  \begin{eqnarray*}
    \Var(\a^T\Y) & = & \a^T\Cov(\Y)\a  \pause\\
 & = & \sigma^2 \| \a \|^2  \pause \\
& = & \sigma^2 (\| \a^* \|^2 + \|\u\|^2 + 2 \a^{*T} \u) \pause\\
& = & \sigma^2 (\| \a^* \|^2 + \|\u\|^2) + 0 \pause\\
& = & \Var(\a^{*T}\Y) + \sigma^2 \|\u\|^2 \pause \\
& \geq & \Var(\a^{*T}\Y) \pause
  \end{eqnarray*}
with equality if and only if $\a = \a^*$ \pause
\end{itemize}
Hence $\a^{*T}\Y$ is the unique linear unbiased estimator of $\psi$
with minimum variance \pause {\color{blue}''BLUE'' = Best Linear Unbiased Estimator}
  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Continued}
  \begin{proof}
  Show that $\hat{\psi} = \a^{*T}\Y = \lambdab^T\bhat$ \pause

  Since $\a^* \in C(\X)$ we have $\a^*  =  \P_\X \a^*$  \pause
    \begin{eqnarray*}
\a^{*T}\Y & = &  \a^{*T} \P_X^T \Y \pause \\
         & = & \a^{*T}\P_x\Y \pause \\
         & = & \a^{*T} \X \bhat \pause \\
         & = & \lambdab^T \bhat  \pause
    \end{eqnarray*}
for $\lambdab^T = \a^{*T}\X$  or $\lambdab = \X^T\a$
  \end{proof}
\end{frame}
\begin{frame}
  \frametitle{MVUE}
  \begin{itemize}
  \item Gauss-Markov Theorem says that OLS has minimum variance in the
    class of all Linear Unbiased estimators \pause
\item Requires just first and second moments \pause
\item Additional assumption of normality,  OLS = MLEs have
  minimum variance out of \alert<3>{ALL}  unbiased estimators (MVUE); not
  just linear estimators \pause (requires Completeness and
Rao-Blackwell Theorem - next semester) \pause
  \end{itemize}

\end{frame}
\begin{frame} \frametitle{Prediction}
  \begin{itemize}
  \item   For predicting at new $\x_*$ is there always a unique unbiased
  estimator of $E[\Y \mid \x_*]$? \pause
\item If one does exist, how do we know that if we are given $\lambdab$?

  \end{itemize}

\end{frame}
\begin{frame} \frametitle{Existence}
  \begin{itemize}
  \item  $\x_*^T \b$ has a unique unbiased estimator if $\x_* \equiv
    \lambdab = \X^T \a$ \pause
\item Clearly if $\x_* = \x_i$ ($i$th row of observed data) then it is
  estimable with $\a$ equal to the vector with a 1 in the $i$th position
  even if $\X$ is not full rank! \pause
\item What about out of sample prediction? \pause
\end{itemize}

\end{frame}


\begin{frame}[fragile] \frametitle{Example}

\begin{small}
<<>>=
x1 = -4:4
x2 = c(-2, 1, -1, 2, 0, 2, -1, 1, -2)
x3 = 3*x1  -2*x2
x4 = x2 - x1 + 4
Y = 1+x1+x2+x3+x4 + c(-.5,.5,.5,-.5,0,.5,-.5,-.5,.5)
dev.set = data.frame(Y, x1, x2, x3, x4)
lm1234 = lm(Y ~ x1 + x2 + x3 + x4, data=dev.set)
round(coefficients(lm1234), 4)
lm3412 = lm(Y ~ x3 + x4 + x1 + x2, data = dev.set)
round(coefficients(lm3412), 4)
@
\end{small}
\end{frame}

\begin{frame}[fragile] \frametitle{In Sample Predictions}

<<>>=
cbind(dev.set, predict(lm1234), predict(lm3412))
@

Both models agree for estimating the mean at the observed $\X$ points!
\end{frame}
\begin{frame} [fragile] \frametitle{Out of Sample}

<<echo=FALSE>>=
test.set = data.frame(
    x1 = c(3, 6, 6, 0, 0, 1),
    x2 = c(1, 2, 2, 0, 0, 2),
    x3 = c(7,14, 14,0, 0, 3),
    x4 = c(2, 4, 0, 4, 0, 4))

@

<<warning=FALSE>>=
out = data.frame(test.set,
      Y1234=predict(lm1234, new=test.set),
      Y3412=predict(lm3412, new=test.set))
out
@

\pause
Agreement for cases 1, 3, and 4 only!  Can we determine that without
finding the predictions and comparing?
\end{frame}

\begin{frame} \frametitle{Determining Estimable $\lambdab$}
  \begin{itemize}

 \item Estimable means that $\lambdab^T = \a^T\X$  for $\a \in C(\X)$ \pause
 \item Transpose: $\lambdab = \X^T \a$  for $\a \in C(\X)$ \pause
 \item $\lambdab \in C(\X^T)$ ($\lambdab \in R(\X)$)\pause
 \item $\lambdab \perp C(\X^T)^{\perp}$ \pause
 \item $C(\X^T)^{\perp}$ is the null space of $\X$  \pause
$$ \v \perp C(\X^T): \, \X \v = 0 \Leftrightarrow \v \in N(\X)$$ \pause
\item $\lambdab \perp  N(\X)$ \pause
\item if $\P$ is a projection onto $C(\X^T)$ then $\I - \P$ is a projection
  onto $N(\X)$ and therefore $(\I - \P) \lambdab = \zero$ if $\lambdab$
  is estimable \pause
  \end{itemize}
\vfill
Take $\P_{\X^T} = (\X^T\X)(\X^T\X)^{-}$ as a projection onto $C(\X^T)$
and show $(\I - \P_{\X^T})\lambdab = \zero_p$
\end{frame}

\begin{frame}[fragile]\frametitle{Example}
<<>>=
library("estimability" )
cbind(epredict(lm1234, test.set), epredict(lm3412, test.set))
@

Rows 2, 5, and 6 are not estimable!  No linear unbiased estimator


\end{frame}
\begin{frame} \frametitle{Summary}
  \begin{itemize}
  \item When BLUEs exist,  under normality they are MVUE  (ditto for
    prediction - BLUP) \pause
 \item BLUE/BLUP do not always exist for estimation/prediction if $\X$ is
   not full rank \pause
\item may occur with redundancies for modest $p < n$ and of course $p
  > n$ \pause
\item Eliminate redundancies by removing variables (variable
  selection) \pause
\item Consider alternative estimators (Bayes and related)
  \end{itemize}
\end{frame}

\begin{frame} \frametitle{Other Estimators}
What about some estimator $g(\Y)$ that is not unbiased? \pause
  \begin{itemize}
\item Mean Squared Error for estimator $g(\Y)$ of $\lambdab^T\b$ is
$$\E[g(\Y) - \lambdab^T\b]^2 = \Var(g(\Y)) + \text{Bias}^2(g(\Y))$$
where Bias = $\E[g(\Y)] - \lambdab^T\b$  \pause
\item Bias vs Variance tradeoff \pause
\item Can have smaller MSE if we allow some Bias!
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Bayes}
  \begin{itemize}
  \item  Next Class Bayes Theorem \& Conjugate Normal-Gamma Prior/Posterior   distributions
  \item  Read   Chapter 2 in Christensen or Wakefield 5.7
  \item  Review Multivariate Normal and Gamma distributions
  \end{itemize}





\end{frame}
\end{document}

