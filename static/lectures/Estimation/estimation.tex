\documentclass[]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\documentclass[handout]{beamer}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{../macros}
\usepackage{verbatim}

\usetheme{default}
\title{Frequentist Properties of Bayes Estimators}
\institute{Merlise Clyde}
\author{STA721 Linear Models Duke University}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle



\begin{frame}[t]
  \frametitle{Recap: Bayesian Estimation with 2 Block $g$-prior (Normal-Jeffreys)}

\begin{itemize}
  \item

Model in centered parameterization $\X_c = (\I_n - \P_1) \X$
\begin{eqnarray*}
\Y & = & \one \beta_0 + (\I_n - \P_1) \X \b + \eps \\
\Y & = & \one \beta_0 + (\X_c \b + \eps \\
p(\beta_0, \phi) & \propto & 1/\phi\\
\b \mid \beta_0, \phi & \sim & \N(\zero, \frac{g}{\phi} (\X_c^T \X_c)^{-1})
\end{eqnarray*}
\pause

\item Zellner-Siow prior (assume $\X$ is centered) \pause
Zellner's g-prior $\b \mid \phi \sim \N(\zero_p, g
    (\X_c^T\X_c)^{-1}/\phi)$ \pause

\item Let $\tau = 1/g$ assign $\tau \sim G(1/2, n/2)$ \pause
\item Marginal prior on $\b \sim C(0, \phi^{-1} (\X_c^T\X_c/n)^{-1})$ \pause
\item Use Gibbs sampling or MCMC
\end{itemize}
\end{frame}













\begin{frame}[fragile]{JAGS Code: library(R2jags)}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{=} \hlkwa{function}\hlstd{()\{}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n) \{}
      \hlstd{Y[i]} \hlopt{~} \hlkwd{dnorm}\hlstd{(beta0}\hlopt{+} \hlstd{(X[i]} \hlopt{-}\hlstd{Xbar)}\hlopt{*}\hlstd{beta, phi)}
  \hlstd{\}}
  \hlstd{beta0} \hlopt{~} \hlkwd{dnorm}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{.000001}\hlopt{*}\hlstd{phi)} \hlcom{#precision is 2nd arg}
  \hlstd{beta} \hlopt{~} \hlkwd{dnorm}\hlstd{(}\hlnum{0}\hlstd{, phi}\hlopt{*}\hlstd{tau}\hlopt{*}\hlstd{SSX)}  \hlcom{#precision is 2nd arg}
  \hlstd{phi} \hlopt{~} \hlkwd{dgamma}\hlstd{(}\hlnum{.001}\hlstd{,} \hlnum{.001}\hlstd{)}
  \hlstd{tau} \hlopt{~} \hlkwd{dgamma}\hlstd{(}\hlnum{.5}\hlstd{,} \hlnum{.5}\hlopt{*}\hlstd{n)}
  \hlstd{g} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlstd{tau}
  \hlstd{sigma} \hlkwb{<-} \hlkwd{pow}\hlstd{(phi,} \hlopt{-}\hlnum{.5}\hlstd{)}
\hlstd{\}}
\hlstd{data} \hlkwb{=} \hlkwd{list}\hlstd{(}\hlkwc{Y}\hlstd{=Y,} \hlkwc{X}\hlstd{=X,} \hlkwc{n} \hlstd{=}\hlkwd{length}\hlstd{(Y),} \hlkwc{SSX}\hlstd{=}\hlkwd{sum}\hlstd{(Xc}\hlopt{^}\hlnum{2}\hlstd{),}
            \hlkwc{Xbar}\hlstd{=}\hlkwd{mean}\hlstd{(X))}
\hlstd{ZSout} \hlkwb{=} \hlkwd{jags}\hlstd{(data,} \hlkwc{inits}\hlstd{=}\hlkwa{NULL}\hlstd{,}
             \hlkwc{parameters.to.save}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"beta0"}\hlstd{,}\hlstr{"beta"}\hlstd{,} \hlstr{"g"}\hlstd{,}
                                  \hlstr{"sigma"}\hlstd{),}
             \hlkwc{model}\hlstd{=model,} \hlkwc{n.iter}\hlstd{=}\hlnum{10000}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# module glm loaded}}\begin{verbatim}
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 4
##    Total graph size: 174
## 
## Initializing model
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile] \frametitle{HPD intervals}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{confint}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{Xc))}
\end{alltt}
\begin{verbatim}
##                  2.5 %    97.5 %
## (Intercept) -0.3985359 0.2048303
## Xc           2.7945824 3.4555162
\end{verbatim}
\begin{alltt}
\hlkwd{HPDinterval}\hlstd{(}\hlkwd{as.mcmc}\hlstd{(ZSout}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.matrix))}
\end{alltt}
\begin{verbatim}
##               lower        upper
## beta      2.7823047    3.4453690
## beta0    -0.3764027    0.2095465
## deviance 70.2043917   78.4813041
## g        19.4503373 3782.7134974
## sigma     0.6171029    1.0504892
## attr(,"Probability")
## [1] 0.95
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile] \frametitle{Compare}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth,height=2.5in]{figure/unnamed-chunk-3-1} 

\end{knitrout}

\end{frame}

\begin{frame}[fragile]
\begin{tiny}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ZSout}
\end{alltt}
\begin{verbatim}
## Inference for Bugs model at "/var/folders/n4/nj1122xj6bn5_xgbptv7bml40000gp/T//RtmpwDmY3Y/model17fcc74341fa7.txt", fit using jags,
##  3 chains, each with 10000 iterations (first 5000 discarded), n.thin = 5
##  n.sims = 3000 iterations saved
##           mu.vect   sd.vect   2.5%     25%     50%     75%    97.5%  Rhat
## beta        3.112     0.170  2.782   2.997   3.115   3.225    3.445 1.001
## beta0      -0.099     0.152 -0.384  -0.204  -0.099   0.001    0.204 1.002
## g        2263.147 38967.029 48.273 146.129 282.298 697.063 9018.709 1.001
## sigma       0.827     0.114  0.636   0.747   0.816   0.896    1.079 1.001
## deviance   73.347     2.563 70.390  71.458  72.680  74.500   79.882 1.002
##          n.eff
## beta      3000
## beta0     1200
## g         3000
## sigma     3000
## deviance  1600
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.3 and DIC = 76.6
## DIC is an estimate of expected predictive error (lower deviance is better).
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{tiny}
\end{frame}

\begin{frame}[fragile]\frametitle{Posterior Distribution of shrinkage}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=3in,height=3in]{figure/marginal-1} 

\end{knitrout}


\end{frame}

\begin{frame}[fragile] \frametitle{Joint Distribution of $\sigma$ and $g/(1 + g)$}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(postdf,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=sigma,} \hlkwc{y}\hlstd{=shrinkage) )} \hlopt{+}
 \hlkwd{stat_density_2d}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{fill} \hlstd{= ..level..),}
                 \hlkwc{geom} \hlstd{=} \hlstr{"polygon"}\hlstd{,} \hlkwc{colour}\hlstd{=}\hlstr{"white"}\hlstd{)} \hlopt{+}
  \hlkwd{theme_light}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=2in,height=2in]{figure/unnamed-chunk-5-1} 

\end{knitrout}


\end{frame}

\begin{frame}\frametitle{Cauchy Summary}
\begin{itemize}
\item Cauchy rejects prior mean if it is an "outlier"
\item robustness related to "bounded" influence (more later)
\item requires numerical integration or Monte Carlo sampling (MCMC)
\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{How Good are Bayes Estimators?}
Quadratic loss for estimating  $\b$ using estimator $\a$
$$ L(\b, \a) =  ( \b - \a)^T(\b -\a)$$ \pause

\begin{itemize}
\item Consider our expected loss (before we see the data) of taking an
``action'' $\a$ \pause
\item Under OLS or the  Reference prior the Expected Mean Square Error  \pause
  \begin{eqnarray*}
\E_\Y[( \b - \bhat)^T(\b -\bhat) & = &\sigma^2
  \tr[(\X^T\X)^{-1}] \pause \\
 & = & \sigma^2 \sum_{j=1}^p \lambda_j^{-1}
  \end{eqnarray*}
where $\lambda_j$ are eigenvalues of $\X^T\X$.
\pause
\item If smallest $\lambda_j \to 0$ then MSE  $\to \infty$
\item Note: estimate is unbiased!
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Is the $g$-prior better?}

\begin{itemize}
  \item

\item Explore Frequentist properties of using a Bayesian estimator
$$\E_\Y[( \b - \bhat_g)^T(\b -\bhat_g)$$
but now $\bhat_g = g/(1+g) \bhat$  \pause

\item Sampling distribution of $\bhat_g  = \frac{g}{1+g}(\X^\X)^{-1} \X^T\Y$  \pause
\item HW: show that there is a value of $g$ prior such that the g-prior is always better than the Reference prior/OLS

\item Potential problem: MSE also blows up if smallest eigenvalue goes to zero!
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Estimator Properties}

  \begin{itemize}
  \item  Bias  \pause
  \item  Variance \pause
  \item MSE = Bias$^2$ + Variance  (multivariate analogs) \pause
\item Problems with OLS, g-priors \& mixtures of g-priors with collinearity \pause
\item Solutions: \pause
  \begin{itemize}
  \item removal of terms \pause
   \item other shrinkage estimators
  \end{itemize}

  \end{itemize}
\end{frame}






\begin{frame}
  \frametitle{Canonical Representation \& Ridge Regression}


  \begin{itemize}
\item  Assume that $\X$ has been centered and standardized so that $\X^T\X
  = \corr(\X)$ \pause  (use {\tt scale} or {\tt sweep} functions in {\tt R}) \pause
  \item Write $\X = \U_p L \V^T$ Singular Value Decomposition \pause where
    $\U_p^T\U_p = \I_p$ and $\V$ is $p \times p$ orthogonal matrix,
    $L$ is diagonal \pause
  $$ \Y = \one \alpha +  \U_p L \V^T \b + \eps $$ \pause
  \item  Let $\g =  \V^T \b$ and create
   $\U$   an $n \times n$  orthogonal matrix \pause
  $$\U = [\U_0 \, | \, \U_p \,| \,\U_{n-p-1}]$$
  where $\U_0 = \one/\sqrt{n}$  \pause
  \item $\U_0^T\U_p = 0$, $\U^T_0 \U_{n-p-1} = 0$ and  $\U_p^T\U_{n - p -1} = 0$ (orthogonal columns)
  \end{itemize}
\end{frame}
  \begin{frame} {Orthogonal Regression}
Rotate by multiplyting by $\U^T$:
  \begin{eqnarray*}
    \U^T \Y & = & \U^T \one \alpha + \U^T \U_p L \V^T \b + \U^T \eps \pause \\
    \Y^*
   &=& \left[
  \begin{array}{cc}
  \sqrt{n} & \zero_p^T\\ \zero_p & L \\  \zero_{n-p -1} & \zero_{n - p - 1  \times p}
  \end{array}
 \right]
\left(    \begin{array}{c}
      \alpha \\
  \g
    \end{array} \right)
+  \eps^*
  \end{eqnarray*} \pause


  \begin{itemize}
  \item   $y^*_0 \equiv \hat{\alpha} = \ybar$ \pause
\item $\hat{\g} =( L^TL)^{-1} L^T \U^T_p\Y$ or $\hat{\gamma}_i = y^*_i/l_i$ for
  $i = 1, \ldots, p$ \pause
\item  $\Var(\hat{\gamma}_i) = \sigma^2/l_i^2$
  \end{itemize}
Directions in $\X$ space $\U_j$ with small eigenvectors $l_i$ have
the largest variances.  Unstable directions.
\end{frame}

\begin{frame}
  \frametitle{Ridge  Regression \& Independent Prior}
  (Another) Normal Conjugate Prior Distribution on $\g$: $$\g \mid \phi \sim \N(\zero_p, \frac{1}{ \phi
    k} \I_p)$$ \pause

Posterior mean
$$
 \tilde{\g} =( L^TL + k \I)^{-1} L^T \U^T_p\Y  = ( L^TL + k \I)^{-1}
 L^TL\hat{\g}
$$\pause

 $$\tilde{\gamma}_i = \frac{l_i^2}{l_i^2 + k} \hat{\gamma}_i =
 \frac{\lambda_i}{\lambda_i + k} \hat{\gamma}_i $$ \pause
 \begin{itemize}
\item When $\lambda_i \to 0$ then $\tilde{\gamma}_i \to 0$

\item When $k \to 0$ we get OLS back but if $k$ gets too  big posterior
  mean goes to zero.
 \end{itemize}

\end{frame}
\begin{frame}
  \frametitle{Transform}
  \begin{itemize}
  \item
  Transform back $\tilde{\b} = \V \tilde{\g}$ \pause
  $$\tilde{\b} = (\X^T\X + k \I)^{-1}  \X^T\X \bhat$$ \pause
\item importance of standardizing \pause

\item Is there a value of $k$ for which ridge is better in terms of
  Expected MSE than OLS? \pause
\item Choice of $k$?
  \end{itemize}
\end{frame}
\begin{frame}\frametitle{MSE}
Can show that
 $$\E[(\b - \btilde)^T(\b - \btilde)] = \E[(\g - \tilde{\g})^T(\g - \tilde{\g}]$$
  \pause
 \begin{itemize}
 \item $\Var(\tilde{\gamma}_i) = \sigma^2 l_i^2 /(l_i^2 +
   k)^2$ \pause
\item  Bias of $\tilde{\g}$ is $-k \gamma_i/(l_i^2 + k)$ \pause
 \item  MSE $$\sigma^2 \sum_i \frac{l_i^2}{(l_i^2 + k)^2} +  k^2
   \sum_i \frac{\gamma_i^2} {(l_i^2 + k)^2} $$
 \end{itemize}
The derivative with respect to $k$ is negative at $k=0$, hence the
function is decreasing. \pause

\vspace{12pt}
Since $k = 0$ is OLS, this means that is a value of $k$ that will
always be better than OLS
\end{frame}

\begin{frame}  \frametitle{Alternative Motivation}
  \begin{itemize}
  \item If  $\bhat$ is unconstrained  expect high variance with nearly
    singular $\X$ \pause
  \item Let $\Y^c = (\I - \P_1) \Y$  and $\X^c$ the centered and
    standardized  $\X$ matrix \pause
\item Control how large coefficients may grow \pause
    $$\min_{\b} (\Y^c - \X^c \b)^T (\Y^c - \X^c\b)$$
    subject to
    $$ \sum \beta_j^2 \le t$$ \pause
  \item Equivalent Quadratic Programming Problem
    $$\min_{\b} \| \Y^c - \X^c \b\|^2 + k \|\b\|^2$$ \pause
  \item ``penalized'' likelihood \pause
  \end{itemize}
\end{frame}
\begin{frame}\frametitle{Picture}

\end{frame}







\end{document}

