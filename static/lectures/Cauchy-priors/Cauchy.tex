%\documentclass[]{beamer}
\documentclass[handout]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
% maxwidth is the original width if it is less than linewidth
% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%
\let\hlipl\hlkwb

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{../macros}
\usepackage{verbatim}

\usetheme{default}
\title{Cauchy Priors: Mixtures of Normals \& MCMC}
\institute{Merlise Clyde}
\author{STA721 Linear Models Duke University}
\date{\today}
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\maketitle



\begin{frame}[t]
  \frametitle{Bayesian Estimation with 2 Block $g$-prior (Normal-Jeffreys)}


Model in centered parameterization
\begin{eqnarray*}
\Y & = & \one \beta_0 + (\I_n - \P_1) \X_1 \b + \eps \\
p(\beta_0, \phi) & \propto & 1/\phi\\
\b \mid \beta_0, \phi & \sim & \N(\zero, \frac{g}{\phi} (\X^T (\I_n - \P_{\one}) \X)^{-1})
\end{eqnarray*}

Log Likelihood

$${\cal{L}}(\beta_0, \beta_1, \phi) \propto
\frac{n}{2}\log(\phi) - \frac{\phi}{2} \left(
  n (\beta_0 - \ybar)^2
+ (\Y_c - \X_c \b)^T(\Y_c - \X_c)\right)$$

Since
$$\Y = (\I - \P_{\one})\Y  + \P_{\one}\Y$$

Integrated Liklihood after integrating $\beta_0$
$${\cal{L}}(\beta_1, \phi) \propto
\frac{n -1}{2}\log(\phi)
- \frac{\phi}{2} (\Y_c - \X_c \b)^T(\Y_c - \X_c)$$

\end{frame}

\begin{frame}\frametitle{Prior Data}
Note $$(\X^T (\I_n - \P_{\one}) \X) = (\X^T (\I_n - \P_{\one})^T (\I_n
- \P_{\one}) \X)  = (\X -\one_n \Xbar^T)^T (\X - \one_n \Xbar) $$
\pause
Let  $(\X -\one_n \Xbar^T)^T (\X - \one \Xbar) = \SS_{\X} = \U^T \U$
\pause

Quadratic contribution to the log likelihood from prior after integrating out $\beta_0$

$$ ( \Y_c  - \X_c \b)^T(\Y_c - \X_c\b) + ( \b^T\frac{\U^T \U}{g} \b)   $$
\pause
$$ ( \Y_c  - \X_c \b)^T(\Y_c - \X_c\b) + ( \zero_p -
\frac{\U}{\sqrt{g}}\b)^T(\zero_p -  \frac{\U}{\sqrt{g}} \b)   $$

Prior observations  with $Yc =  0$.
\end{frame}


\begin{frame}\frametitle{Example: g=5, n=30}

In SLR it is like an extra $Y_0 = 0$ at $\X_o = \sqrt{\frac{\SS_x}{g}}$:
$$ ( \Y_c  - \X_c \b)^T(\Y_c - \X_c\b) + (0 - \sqrt{\frac{\SS_x}{g}}\b)^T (0 - \sqrt{\frac{\SS_X}{g}} \b)   $$
\pause


\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth,height=2.5in]{figure/unnamed-chunk-1-1} 

\end{knitrout}

\end{frame}



\begin{frame}
  \frametitle{Disadvantages of Conjugate Priors}
  Disadvantages: \pause
\begin{itemize}
\item Results  may have be sensitive to prior ``outliers'' due to
  linear updating \pause
\item Problem potentially with all Normal priors, not just the
  $g$-prior. \pause
\item Cannot capture all possible prior beliefs \pause
\item Mixtures of Conjugate Priors
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Mixtures of Conjugate Priors}
  \begin{theorem}[Diaconis \& Ylivisaker 1985]  Given a sampling model
  $p(y \mid \tb)$ from an exponential family, any prior distribution
  can be expressed as a mixture of conjugate prior distributions
 \end{theorem}

 \begin{itemize}
 \item Prior $p(\tb) = \int p(\tb \mid \omega) p(\omega)\, d \omega$ \pause
 \item Posterior \pause
   \begin{eqnarray*}
   p(\tb \mid \Y)  &\propto & \int p(\Y \mid \tb) p(\tb\mid \omega)
   p(\omega) \, d\omega \pause \\
 & \propto & \int  \frac{  p(\Y \mid \tb) p(\tb \mid \omega) } {p(\Y \mid
   \omega)}  p(\Y \mid
 \omega) p(\omega ) \, d \omega  \pause \\
& \propto & \int p(\tb \mid \Y, \omega)  p(\Y \mid
 \omega) p(\omega ) \, d \omega \pause \\
 p(\tb \mid \Y) & =  & \frac{\int p(\tb \mid \Y, \omega)  p(\Y \mid
 \omega) p(\omega ) \, d \omega }
{\int p(\Y \mid
 \omega) p(\omega ) \, d \omega }
       \end{eqnarray*}

 \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Zellner-Siow prior (assume $\X$ is centered)}
Zellner's g-prior $\b \mid \phi \sim \N(\zero_p, g
    (\X_c^T\X_c)^{-1}/\phi)$ \pause

\begin{itemize}
\item Choice of $g$?  \pause
\item $\frac{g}{1 + g}$  weight given to the data \pause
\item Let $\tau = 1/g$ assign $\tau \sim G(1/2, n/2)$ \pause
\item Marginal prior on $\b \sim C(0, \phi^{-1} (\X_c^T\X_c/n)^{-1})$
\item Can express posterior as a mixture of $g$-priors
$$p(\tau \mid \Y) = \frac{p(\Y \mid \tau) p(\tau)}{\int p(\Y \mid \tau) p(\tau) \, d \tau}$$ \pause
\item Problem:  no analytic expression for integral \pause
\item Need 2 one dimensional integrals to obtain posterior. \pause
\item What about credible intervals?
\end{itemize}
\end{frame}

\begin{frame}[t] \frametitle{Markov Chain Monte Carlo}
\begin{itemize}
\item We know that $\beta_0, \b, \phi \mid \Y, g=1/\tau$ has a Normal-Gamma distribution  \pause
\item We can show that $\tau \mid \beta_0,\b, \phi, \Y$ has a Gamma distribution \pause
$$p(\tau \mid \b, \phi, \Y) \propto {\cal{L}}(\beta_0,\b, \phi)
\tau^{p/2} e^{( - \tau \frac{\phi}{2} \b^T (\X^T\X) \b)}
\tau^{1/2 - 1} e^{ - \tau n/2}$$ \pause
\item alternate sampling from full conditional distributions given current values of other parameters.  (STA 601) \pause
\item JAGS or STAN
\end{itemize}
\end{frame}


\begin{frame}[fragile]{JAGS Code: library(R2jags)}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{model} \hlkwb{=} \hlkwa{function}\hlstd{()\{}
  \hlkwa{for} \hlstd{(i} \hlkwa{in} \hlnum{1}\hlopt{:}\hlstd{n) \{}
      \hlstd{Y[i]} \hlopt{~} \hlkwd{dnorm}\hlstd{(beta0}\hlopt{+} \hlstd{(X[i]} \hlopt{-}\hlstd{Xbar)}\hlopt{*}\hlstd{beta, phi)}
  \hlstd{\}}
  \hlstd{beta0} \hlopt{~} \hlkwd{dnorm}\hlstd{(}\hlnum{0}\hlstd{,} \hlnum{.000001}\hlopt{*}\hlstd{phi)} \hlcom{#precision is 2nd arg}
  \hlstd{beta} \hlopt{~} \hlkwd{dnorm}\hlstd{(}\hlnum{0}\hlstd{, phi}\hlopt{*}\hlstd{tau}\hlopt{*}\hlstd{SSX)}  \hlcom{#precision is 2nd arg}
  \hlstd{phi} \hlopt{~} \hlkwd{dgamma}\hlstd{(}\hlnum{.001}\hlstd{,} \hlnum{.001}\hlstd{)}
  \hlstd{tau} \hlopt{~} \hlkwd{dgamma}\hlstd{(}\hlnum{.5}\hlstd{,} \hlnum{.5}\hlopt{*}\hlstd{n)}
  \hlstd{g} \hlkwb{<-} \hlnum{1}\hlopt{/}\hlstd{tau}
  \hlstd{sigma} \hlkwb{<-} \hlkwd{pow}\hlstd{(phi,} \hlopt{-}\hlnum{.5}\hlstd{)}
\hlstd{\}}
\hlstd{data} \hlkwb{=} \hlkwd{list}\hlstd{(}\hlkwc{Y}\hlstd{=Y,} \hlkwc{X}\hlstd{=X,} \hlkwc{n} \hlstd{=}\hlkwd{length}\hlstd{(Y),} \hlkwc{SSX}\hlstd{=}\hlkwd{sum}\hlstd{(Xc}\hlopt{^}\hlnum{2}\hlstd{),}
            \hlkwc{Xbar}\hlstd{=}\hlkwd{mean}\hlstd{(X))}
\hlstd{ZSout} \hlkwb{=} \hlkwd{jags}\hlstd{(data,} \hlkwc{inits}\hlstd{=}\hlkwa{NULL}\hlstd{,}
             \hlkwc{parameters.to.save}\hlstd{=}\hlkwd{c}\hlstd{(}\hlstr{"beta0"}\hlstd{,}\hlstr{"beta"}\hlstd{,} \hlstr{"g"}\hlstd{,}
                                  \hlstr{"sigma"}\hlstd{),}
             \hlkwc{model}\hlstd{=model,} \hlkwc{n.iter}\hlstd{=}\hlnum{10000}\hlstd{)}
\end{alltt}


{\ttfamily\noindent\itshape\color{messagecolor}{\#\# module glm loaded}}\begin{verbatim}
## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 30
##    Unobserved stochastic nodes: 4
##    Total graph size: 174
## 
## Initializing model
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile] \frametitle{HPD intervals}



\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{confint}\hlstd{(}\hlkwd{lm}\hlstd{(Y} \hlopt{~} \hlstd{Xc))}
\end{alltt}
\begin{verbatim}
##                  2.5 %    97.5 %
## (Intercept) -0.3985359 0.2048303
## Xc           2.7945824 3.4555162
\end{verbatim}
\begin{alltt}
\hlkwd{HPDinterval}\hlstd{(}\hlkwd{as.mcmc}\hlstd{(ZSout}\hlopt{$}\hlstd{BUGSoutput}\hlopt{$}\hlstd{sims.matrix))}
\end{alltt}
\begin{verbatim}
##               lower        upper
## beta      2.7823047    3.4453690
## beta0    -0.3764027    0.2095465
## deviance 70.2043917   78.4813041
## g        19.4503373 3782.7134974
## sigma     0.6171029    1.0504892
## attr(,"Probability")
## [1] 0.95
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{frame}

\begin{frame}[fragile] \frametitle{Compare}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth,height=2.5in]{figure/unnamed-chunk-4-1} 

\end{knitrout}

\end{frame}

\begin{frame}[fragile]
\begin{tiny}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlstd{ZSout}
\end{alltt}
\begin{verbatim}
## Inference for Bugs model at "/var/folders/n4/nj1122xj6bn5_xgbptv7bml40000gp/T//RtmpwzlWaF/model8022471112d1.txt", fit using jags,
##  3 chains, each with 10000 iterations (first 5000 discarded), n.thin = 5
##  n.sims = 3000 iterations saved
##           mu.vect   sd.vect   2.5%     25%     50%     75%    97.5%  Rhat
## beta        3.112     0.170  2.782   2.997   3.115   3.225    3.445 1.001
## beta0      -0.099     0.152 -0.384  -0.204  -0.099   0.001    0.204 1.002
## g        2263.147 38967.029 48.273 146.129 282.298 697.063 9018.709 1.001
## sigma       0.827     0.114  0.636   0.747   0.816   0.896    1.079 1.001
## deviance   73.347     2.563 70.390  71.458  72.680  74.500   79.882 1.002
##          n.eff
## beta      3000
## beta0     1200
## g         3000
## sigma     3000
## deviance  1600
## 
## For each parameter, n.eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor (at convergence, Rhat=1).
## 
## DIC info (using the rule, pD = var(deviance)/2)
## pD = 3.3 and DIC = 76.6
## DIC is an estimate of expected predictive error (lower deviance is better).
\end{verbatim}
\end{kframe}
\end{knitrout}
\end{tiny}
\end{frame}

\begin{frame}[fragile]\frametitle{Posterior Distribution of shrinkage}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=3in,height=3in]{figure/marginal-1} 

\end{knitrout}


\end{frame}

\begin{frame}[fragile] \frametitle{Joint Distribution of $\sigma$ and $g/(1 + g)$}

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlkwd{ggplot}\hlstd{(postdf,} \hlkwd{aes}\hlstd{(}\hlkwc{x}\hlstd{=sigma,} \hlkwc{y}\hlstd{=shrinkage) )} \hlopt{+}
 \hlkwd{stat_density_2d}\hlstd{(}\hlkwd{aes}\hlstd{(}\hlkwc{fill} \hlstd{= ..level..),}
                 \hlkwc{geom} \hlstd{=} \hlstr{"polygon"}\hlstd{,} \hlkwc{colour}\hlstd{=}\hlstr{"white"}\hlstd{)} \hlopt{+}
  \hlkwd{theme_light}\hlstd{()}
\end{alltt}
\end{kframe}
\includegraphics[width=2in,height=2in]{figure/unnamed-chunk-6-1} 

\end{knitrout}


\end{frame}

\begin{frame}\frametitle{Cauchy Summary}
\begin{itemize}
\item Cauchy rejects prior mean if it is an "outlier"
\item robustness related to "bounded" influence (more later)
\item requires numerical integration or Monte Carlo sampling (MCMC)
\end{itemize}
\end{frame}



\begin{frame}
  \frametitle{How Good are these Estimators?}
Quadratic loss for estimating  $\b$ using estimator $\a$
$$ L(\b, \a) =  ( \b - \a)^T(\b -\a)$$ \pause

\begin{itemize}
\item Consider our expected loss (before we see the data) of taking an
``action'' $\a$ \pause
\item Under OLS or the  Reference prior the Expected Mean Square Error  \pause
  \begin{eqnarray*}
\E_\Y[( \b - \bhat)^T(\b -\bhat) & = &\sigma^2
  \tr[(\X^T\X)^{-1}] \pause \\
 & = & \sigma^2 \sum_{j=1}^p \lambda_j^{-1}
  \end{eqnarray*}
where $\lambda_j$ are eigenvalues of $\X^T\X$.
\pause
\item If smallest $\lambda_j \to 0$ then MSE  $\to \infty$
\item Note: estimate is unbiased!
\end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Is the $g$-prior better?}

Explore Frequentist properties of using a Bayesian estimator

$$\E_\Y[( \b - \bhat_g)^T(\b -\bhat_g)$$

but now $\bhat_g = g/(1+g) \bhat$  \pause

\vfill when is the $g$ prior better than the Reference prior or OLS?  Is it always better?
\end{frame}

\begin{frame}\frametitle{Estimator Properties}

  \begin{itemize}
  \item  Bias  \pause
  \item  Variability \pause
  \item MSE = Bias$^2$ + Variance  (multivariate analogs) \pause
\item Problems with OLS, g-priors \& mixtures of g-priors with collinearity \pause
\item Solutions: \pause
  \begin{itemize}
  \item removal of terms \pause
   \item other shrinkage estimators
  \end{itemize}

  \end{itemize}
\end{frame}














\end{document}

