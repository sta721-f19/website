\documentclass{beamer}
%\documentclass[handout]{beamer}
%\usepackage[dvips]{color}
\usepackage{graphicx, hyperref}
\usepackage{amsmath,amssymb,array,comment,eucal}
\input{../macros}
%\usepackage{verbatim}

\usetheme{default}
\setbeamercolor{structure}{fg=cyan!90!black}

\title{Multivariate Normal Theory}
\institute{Merlise Clyde}
\author{STA721 Linear Models Duke University}
\date{September 4, 2019}
\logo{duke.eps}

\begin{document}
\maketitle

\begin{frame} \frametitle{Outline}

  \begin{itemize}

  \item Multivariate Normal Distribution Singular Case
  \item Equal in Distribution
  \item Conditional Normal Distributions

  \end{itemize}
\end{frame}


\section{Multivariate Normal}

\begin{comment}
\begin{frame}
  \frametitle{Univariate Normal}

  \begin{definition}
  We say that $Z$ has a standard  Normal distribution
$$Z \sim  \N(0,1)$$ with mean 0 and variance 1 if it has density \pause
$$
f_Z(z) = \frac{1}{\sqrt{2 \pi}} e^{-\frac 1 2 z^2}
$$
  \end{definition} \pause

If $Y = \mu + \sigma  Z$ then $Y \sim N(\mu, \sigma^2)$
with mean $\mu$ and variance $\sigma^2$  \pause
$$
f_Y(y) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac 1 2 \left(\frac{z - \mu}{\sigma}\right)^2}
$$

\end{frame}

\begin{frame}
  \frametitle{Standard Multivariate Normal}
Let  $z_i \simiid \N(0, 1)$ for $i = 1, \ldots, d$ and  define
 $$\Z \equiv \left[
  \begin{array}{c}
    z_1 \\ z_2 \\ \vdots \\ z_d
  \end{array}
\right]
$$ \pause


 \begin{itemize}
 \item Density of $Z$: \pause
\begin{eqnarray*}
f_{\Z}(\z) & = \prod_{j=1}^d \frac{1}{ \sqrt{2 \pi}} e^{-z_i^2/2}  \pause \\
 \, & = (2\pi)^{-d/2}e^{- \frac{1}{2} (\Z^T\Z)} \pause
\end{eqnarray*}
\item $\E[\Z] = \zero$ and $\Cov[\Z] =  \I_d$   \pause
\item $\Z \sim \N(\zero_d, \I_d)$ \pause

\end{itemize}
\end{frame}

\frame{ \frametitle{Multivariate Normal}

  For a $d$ dimensional multivariate normal random vector, we write
  $\Y \sim N_d(\mub, \Sigmab)$ \pause

  \begin{itemize}
  \item<2-> $\E[\Y] = \mub$:  $d$ dimensional vector with means
    $E[Y_j]$ \pause
  \item<3-> $\Cov[\Y] = \Sigmab$: $d \times d$ matrix with diagonal elements
    that are the variances of $Y_j$ and off diagonal elements that are
    the covariances $\E[(Y_j - \mu_j)(Y_k - \mu_k)]$ \pause
  \end{itemize}
 \only<4>{ \begin{block}{Density}
 If $\Sigmab$ is positive definite ($\x'\Sigmab \x > 0$ for any $\x \ne
  0$ in $\bbR^d$) then $\Y$ has  a density \footnote{ with respect to Lebesgue
  measure on $\bbR^d$} \pause

$$p(\Y) = (2 \pi)^{-d/2} |\Sigmab|^{-1/2} \exp(-\frac{1}{2}(\Y - \mub)^T
\Sigmab^{-1} (\Y - \mub))$$
  \end{block}}
}




\frame{ \frametitle{Multivariate Normal Density}
\begin{itemize}
 \item<1-> Density of $Z \sim \N(\zero, \I_d)$:
\begin{eqnarray*}
\onslide<2->{f_{\Z}(\z) & = \prod_{j=1}^d \frac{1}{ \sqrt{2 \pi}} e^{-z_i^2/2} } \\
\onslide<3->{ \, & = (2\pi)^{-d/2}e^{- \frac{1}{2} (\Z^T\Z)}}
\end{eqnarray*}
\item<4->  Write $\Y = \mub + \A \Z$
\item<5-> Solve for $\Z = g(\Y)$
\item<6-> Jacobian of the
  transformation  $J(\Z \rightarrow \Y) = | \frac{\partial
    g}{\partial \Y} |$
\item<7>substitute  $g(\Y)$ for $\Z$ in density and
  multiply by Jacobian
{$$  f_\Y(\y)  = f_\Z(\z) J(\Z \rightarrow \Y) $$}
\end{itemize}
}
\frame{ \frametitle{Multivariate Normal Density}
   \begin{equation} \Y  = \mub + \A \Z   \quad    \text{ for }  \Z \sim \N(\zero, \I_d) \end{equation}

  \begin{proof}
  \begin{itemize}
   \item since  $\Sigmab > 0, \, \exists$ by the spectral theorem
   an $\A$ ($d \times d$) such that $\A > 0$ and $\A
  \A^T = \Sigmab$ \pause
  a (symmetric) square root of $\A > 0$ is $\U \Lambdab^{1/2}\U^T$ \pause
\item { $\A >0  \Rightarrow   \A^{-1}$ exists} \pause
\item  Multiply both sides (1)  by $\A^{-1}$: $$\alert{\A^{-1}}\Y =
  \alert{\A^{-1}} \mub + \alert{\A^{-1}}\A \Z$$ \pause
\item substitute $\A^{-1} (\Y - \mub) = \Z $ \pause
\item Jacobian of transformation $d\Z = |\A^{-1}| d\Y$ \pause
  \end{itemize}
{$$f(\Y) = (2 \pi)^{-d/2} |\Sigmab|^{-1/2} \exp(-\frac{1}{2}(\Y - \mub)^T
\Sigmab^{-1} (\Y - \mub))$$  }
  \end{proof}

}

\end{comment}
\begin{frame} \frametitle{Singular Case}
$\Y = \mub + \A \Z$  with $\Z \in \bbR^d$ and $\A$ is $n \times d$ \pause
\begin{itemize}
\item $\E[\Y] = \mu$ \pause
\item $\Cov(\Y) = \A \A^T \ge 0$ \pause
\item $\Y \sim \N(\mub, \Sigmab)$ where $\Sigmab = \A \A^T$
\end{itemize}
  If $\Sigmab$ is singular then there is no density (on $\bbR^n$), but claim that
  $\Y$ still has a multivariate normal distribution!  \pause

 \begin{definition}
  $\Y \in \bbR^n$ has a  multivariate normal distribution $\N(\mub,
  \Sigmab)$ if for any $\v \in \bbR^n$ $\v^T\Y$ has a univariate normal
  distribution with mean $\v^T\mub$ and variance $\v^T\Sigmab \v$
  \end{definition} \pause

Proof:  need momemt generating or characteristic functions which uniquely characterize distribution.
\end{frame}

\begin{frame}{Moment Generating Functions and Characteristics Functions}


Univariate $Y \sim \N(\mu, \sigma^2)$  \pause
\begin{itemize}
  \item MGF or Laplace Transform
  $$m_{Y}(t) = \E[e^{t^TY}]   = \int e^{t^Ty} f(y)  dy
= e^{t^T\mu + \frac{1}{2} t^2 \sigma^2 }
  $$
 \pause
  \item Characteristic function or Fourier transform
  $$\varphi_{Y}(t) = \E[e^{i t^TY}]
  = \int e^{ i t^Ty} f(y)  dy
= e^{i t^T\mu  - \frac{1}{2} t^2 \sigma^2 }
  $$
\end{itemize}
 \pause

To show that $\Y = \mub + \A \Z$ has a $\N(\mub, \Sigmab)$ distribution, we need to show that
$\v^T\Y$ has a MGF or Characteristic function of a univariate normal with mean $\v^T\mub$ and variance $\v^T\Sigma^b\v$.
\end{frame}

\begin{frame}{Proof}
\begin{align*}
\E[e^{i t\v^T\Y}] & =\E[e^{i t \v^T(\mub + \A \Z)}]  \\
                & = e^{i t \v^T{\mub}} \E[e^{it \v^T\A \Z}]  \\
                & =  e^{i t \v^T{\mub}} \E[e^{it \u^T \Z}] \text{ for } \u = \A^T\v \in \bbR^n  \\
                & =  e^{i t \v^T{\mub}} \prod_{j = i}^n\E[e^{it u_j Z_j}]  \\
                 & =  e^{i t \v^T{\mub}} \prod \varphi(t u_j)   \\
                   & =  e^{i t \v^T{\mub}} \prod e^{- \frac 1 2 t^2 u_j^2}  \\
                   & e^{i t \v^T \mub   - \frac 1 2 \sum_j t^2 u_j^2}   \\
                   & e^{i t \v^T \mub - \frac 1 2 t^2 \u^T\u}  \text{ note: } \u^T\u = \v^T\A\A^T\v = \v^T\Sigmab \v \\
                   &  e^{i t \v^T \mub - \frac 1 2 t^2 \v^T\Sigmab \v }
\end{align*}
 \pause
Let $\t = t \v$ then $\varphi(t \v^T \Y) = \varphi(\t^T \Y)$ yields multivariate normal mgf or characteristic function.
\end{frame}

\begin{frame} \frametitle{Multivariate Normal Moment Generating and Characteristic Functions}

$\Y \sim \N(\mub, \Sigmab)$

\begin{itemize}
  \item MGF or Laplace Transform
  $$m_{\Y}(\t) = \E[e^{\t^T\Y}]
  = \int e^{\t^T\y} f(\y)  d\y
= e^{\t^T\mub + \frac{1}{2} \t^T \Sigmab \t}
  $$

  \item Characteristic function (or Fourier Transform)
  $$\phi_{\Y}(\t) = \E[e^{i \t^T\Y}]
  = \int e^{ i \t^T\y} f(\y)  d\y
= e^{i \t^T\mub  - \frac{1}{2} \t^T \Sigmab \t}
  $$
\end{itemize}
\end{frame}

\begin{frame} \frametitle{Linear Transformations are Normal}

If $\Y \sim \N_n(\mub, \Sigmab)$  \pause then for $\A$ $m \times n$

$$\A \Y \sim \N_m(\A \mub, \A \Sigmab \A^T)$$ \pause


$\A \Sigmab \A^T$ does not have to be positive definite! \pause

(Proof in book or linked video uses characteristic functions or MGFs)


\vspace{.5in}

Use to prove that all univariate and multivariate  marginal distributions of normals are normal!


\end{frame}



\begin{frame}
  \frametitle{Equal in Distribution}
  Multiple ways to define the same normal: \pause

  \begin{itemize}
  \item
$\Z_1 \sim \N(\zero, \I_n)$, $\Z_1 \in \bbR^n$  and take $\A$ $d
\times n$ \pause
\item $\Z_2 \sim \N(\zero, \I_p)$,  $\Z_2 \in \bbR^p$  and take $\B$ $d
\times p$ \pause
\item Define $\Y = \mub + \A \Z_1$ \pause
\item Define $\W = \mub + \B \Z_2$ \pause
  \end{itemize}
  \begin{theorem}
    If  $\Y = \mub + \A \Z_1$ and $\W = \mub + \B \Z_2$ then $\Y
    \eqindis \W$ if and only if $\A \A^T = \B \B^T = \Sigmab$
  \end{theorem} \pause
  see linked video
\end{frame}

\frame{ \frametitle{Zero Correlation and Independence}
 \begin{theorem}
For a random vector $\Y \sim \N(\mub, \Sigmab) $ partitioned as
$$
\Y = \left[
  \begin{array}{c}
\Y_1  \\ \Y_2 \end{array} \right]  \sim \N\left( \left[
  \begin{array}{c} \mub_1  \\ \mub_2 \end{array} \right],
  \left[ \begin{array}{cc}
\Sigmab_{11} &  \Sigmab_{12}  \\
\Sigmab_{21} & \Sigmab_{22} \end{array} \right]
 \right)
 $$  \pause
then $\Cov(\Y_1, \Y_2) = \Sigmab_{12} = \Sigmab_{21}^T = \zero$  if and
only if $\Y_1$ and $\Y_2$ are independent.
  \end{theorem}
}

\frame { \frametitle{Independence Implies Zero Covariance}
\begin{proof}
$$ \Cov(\Y_1, \Y_2) = \E[ (\Y_1 - \mub_1)(\Y_2 - \mub_2)^T]$$ \pause
  If $\Y_1$ and $\Y_2$ are independent \pause
$$\E[ (\Y_1 - \mub_1)(\Y_2 - \mub_2)^T] = \E[ (\Y_1 - \mub_1) \E(\Y_2 -
\mub_2)^T] = \zero \zero^T = \zero $$ \pause

therefore $\Sigmab_{12} = \zero$

\end{proof}

}

\frame { \frametitle{ Zero Covariance Implies  Independence}
  Assume $\Sigmab_{12} = \zero$
\begin{block}{Proof}
\begin{itemize}

\item  Choose an $$
  \A = \left[
  \begin{array}{ll}
    \A_1 & \zero \\
    \zero & \A_2
  \end{array}
\right]  $$
 such that $\A_1 \A_1^T = \Sigmab_{11}$, $\A_2 \A_2^T = \Sigmab_{22}$
 \pause
 \item Partition  $$
\Z = \left[
  \begin{array}{c}
    \Z_1 \\ \Z_2
  \end{array}
\right] \sim \N\left(
\left[
  \begin{array}{c}
    \zero_1 \\ \zero_2
  \end{array}
\right],
\left[
  \begin{array}{ll}
    \I_1 &\zero \\
\zero & \I_2
  \end{array}
\right]
 \right)  \text{ and } \mub = \left[
  \begin{array}{c}
    \mub_1 \\ \mub_2
  \end{array}
\right] $$ \pause
\item then
       $\Y \eqindis \A \Z + \mub \sim  \N(\mub, \Sigmab)$
\end{itemize}
  \end{block}
}
\frame {\frametitle{Continued}
  \begin{proof}
    $$
\left[
  \begin{array}{c}
    \Y_1 \\ \Y_2
  \end{array}
\right]  \eqindis \left[
  \begin{array}{c}
    \A_1\Z_1 + \mub_1 \\ \A_2\Z_2 +\mub_2
  \end{array}
\right]
$$ \pause
 \begin{itemize}
\item But $\Z_1$ and $\Z_2$ are independent \pause
\item Functions of $\Z_1$ and $\Z_2$ are independent \pause
\item Therefore $\Y_1$ and $\Y_2$ are independent  \pause
    \end{itemize}
  \end{proof}
For Multivariate Normal Zero Covariance implies independence

}
\frame { \frametitle{ Corollary}
  \begin{corollary}
    If $\Y \sim \N( \mub, \sigma^2 \I_n) $ and $\A \B^T = \zero$
then $\A \Y$ and $\B \Y$ are independent.
  \end{corollary} \pause
  \begin{proof}

$$
\left[
  \begin{array}{c}
    \W_1 \\ \W_2
  \end{array}
\right]  = \left[
  \begin{array}{c}
    \A \\ \B
  \end{array}
\right]  \Y =  \left[
  \begin{array}{c}
    \A \Y \\ \B  \Y
  \end{array}
\right]
$$ \pause
\begin{itemize}
\item $\Cov(\W_1, \W_2) = \Cov(\A \Y, \B \Y) = \sigma^2 \A \B^T$
  \pause
\item $\A \Y $ and $\B \Y$ are independent if $\A \B^T = \zero$
\end{itemize}
  \end{proof}

}




\frame{ \frametitle{Conditional Distributions}

  \begin{Theorem}
If  joint distribution of $\Y_1$ and $\Y_2$ is  $$
\Y = \left[
  \begin{array}{c}
\Y_1  \\ \Y_2 \end{array} \right]  \sim \N\left( \left[
  \begin{array}{c} \mub_1  \\ \mub_2 \end{array} \right],
  \left[ \begin{array}{cc}
\Sigmab_{11} &  \Sigmab_{12}  \\
\Sigmab_{21} & \Sigmab_{22} \end{array} \right]
 \right)
 $$
 and $\Sigma_{22} > 0$
then
$$\Y_1 \mid \Y_2 = \y_2 \sim \N\left( \mub_1 + \Sigmab_{12}
\Sigmab_{22}^{-1} (\y_2 - \mub_2), \Sigmab_{11} - \Sigmab_{12}
\Sigmab_{22}^{-1} \Sigmab_{21}\right)$$
 \end{Theorem}\pause

 \begin{itemize}
 \item
The conditional distribution of $\Y_1$ given $\Y_2$ is also normal!
\pause

\item Can replace $\Sigmab_{22}^{-1}$ by a Generalized inverse if
$\Sigmab_{22}$ is singular.
 \end{itemize}

Brute Force (full rank case) or Linear Transformations!
}

\begin{frame}
  \frametitle{Derivation}
  \begin{block}{Proof}
    \begin{itemize}
    \item Define
$$  \left[
  \begin{array}{c}
\W_1  \\ \W_2 \end{array} \right] =  \left[ \begin{array}{cc}
\I & -\Sigmab_{12} \Sigmab_{22}^{-1}  \\ \zero & \I \end{array} \right]  \left[
  \begin{array}{c}
\Y_1  \\ \Y_2 \end{array} \right] =  \left[
  \begin{array}{c}
\Y_1 - \Sigmab_{12} \Sigmab_{22}^{-1} \Y_2  \\ \Y_2 \end{array} \right]
 $$
\pause
\item  then
$$\W_1 \sim \N\left( \mub_1 - \Sigmab_{12} \Sigmab_{22}^{-1} \mub_2,
\Sigmab_{11} - \Sigmab_{12}\Sigmab_{22}^{-1} \Sigmab_{21} \right) $$ \pause
$$\W_2 \sim \N(\mub_2, \Sigmab_{22})$$ \pause
$$\Cov(\W_1, \W_2) = \zero$$
  \end{itemize}
  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Covariance of $\W_1$ and $\W_2$}
  \begin{block}{}

    $$\Cov(\W_1, \W_2) = \left[\I \, -\Sigmab_{12}
        \Sigmab_{22}^{-1} \right]   \left[ \begin{array}{cc}
\Sigmab_{11} &  \Sigmab_{12}  \\
\Sigmab_{21} & \Sigmab_{22} \end{array} \right] \left[
\begin{array}{c} \zero \\ \I \end{array}
\right] $$
  \end{block}

  \vspace{3in}
\end{frame}


\begin{frame}
  \frametitle{Conditional Characteristic Function }
  \begin{block}{}
    \begin{itemize}
    \item
    $\varphi_{\Y_1 \mid \Y_2 = \y_2}(t) = \E \left[ e^{i t^T \Y_1} \mid \Y_2 =
      \y_2 \right]$ \pause
\item Add zero
$$
 = \E \left[ e^{i t^T \Y_1  \alert<2>{-i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \Y_2 +
     i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \Y_2 } } \mid \Y_2 =
      \y_2 \right]
$$  \pause
\item Factor and exploit conditioning
  \begin{eqnarray*}
&  = \E \left[ e^{i t^T \alert<4>{\Y_1  -i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \Y_2}}
      \,  e^{
     i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \Y_2 }  \mid \Y_2 =
      \y_2 \right]  \pause
\\
&  = \E \left[ e^{i t^T \W_1} \mid \Y_2 = \y_2 \right] \alert<4>{e^{
     i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \y_2 }}
  \end{eqnarray*} \pause
\item Independence of
   $\W_1 = \Y_1 - \Sigma_{12}\Sigma_{22}^{-1}$ and $\Y_2 = \W_2$
$$ = \E \left[ e^{i t^T \W_1} \right]\,  e^{
     i t^T \Sigmab_{12}\Sigmab_{22}^{-1} \y_2 }  $$
   \end{itemize}

  \end{block}
\end{frame}
\begin{frame}
  \frametitle{Combine}
  \begin{block}{}
    \begin{itemize}
    \item $\W_1 \sim \N( \mub_1 - \Sigmab_{12} \Sigmab_{22}^{-1}
      \mub_2, \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})$ \pause
$$\varphi_{\W_1}(t) = e^{i t^T( \mub_1 - \Sigmab_{12} \Sigmab_{22}^{-1}
      \mub_2) - \frac{1}{2} t^T (\Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})t}$$ \pause
\item Combining
  \begin{eqnarray*}
\varphi_{\Y_1 \mid \Y_2} (t)  =  & \varphi_{\W_1}(t) \, e^{  i t^T
    \Sigmab_{12}\Sigmab_{22}^{-1} \y_2 } \pause \\
    = & e^{i t^T( \mub_1 - \Sigmab_{12} \Sigmab_{22}^{-1}
      \mub_2) - \frac{1}{2} t^T (\Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})t} \, e^{  i t^T
    \Sigmab_{12}\Sigmab_{22}^{-1} \y_2 } \pause   \\
 =  & e^{i t^T( \mub_1 + \Sigmab_{12} \Sigmab_{22}^{-1}( \y_2 -
      \mub_2) - \frac{1}{2} t^T (\Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})t}
  \end{eqnarray*} \pause
\item Characteristic function implies
$$
\Y_1 \mid \Y_2 \sim \N( \mub_1 + \Sigmab_{12} \Sigmab_{22}^{-1}( \y_2 -
      \mub_2),  \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})
$$
  \end{itemize}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Regression setting}
  Let $\Y_1 = Y$ and  $\Y_2 = \x$

Then
$$
Y \mid \X \sim \N( \mub_1 + \Sigmab_{12} \Sigmab_{22}^{-1}( \x -
      \mub_2),  \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})
$$

$$
Y \mid \X \sim \N( \mub_1 - \Sigmab_{12} \Sigmab_{22}^{-1} \mu_2  + \Sigmab_{12} \Sigmab_{22}^{-1} \x ,  \Sigmab_{11} - \Sigmab_{12} \Sigmab_{22}^{-1}
      \Sigmab_{21})
$$

$$
Y_i \mid \X \sim \N( \beta_0  + \x_i^T \b ,  \sigma^2)
$$

Multivariate Normality is not necessary
\end{frame}

\begin{frame}
  \frametitle{General Definition }
  \begin{definition}
  Let $\V$ be an vector  space with inner product $\langle ,
  \rangle$.  Then
  $\Y \in \V$ has a  multivariate normal distribution $\N(\mub,
  \Sigmab)$ if for any $\v \in \V$, $\langle \v, \Y \rangle$ has a normal
  distribution with mean $\langle \v,\mub \rangle $ and variance
  $\langle \v,  \Sigmab \v \rangle$
  \end{definition} \pause

For usual Euclidean space inner product $\langle \x, \y \rangle =
\x^T\y$ \pause

\vfill
For the energetic Student:  Consider space of $n \times m$ matrices,
and a random matrix $\Y \sim \N(\mub, \I \otimes \Sigmab)$
where  $(\I \otimes \Sigmab) M = \I M \Sigmab^T$ for $M$ $n \times m$
\pause
\vfill
Under the Inner product $\langle \x, \y \rangle \equiv \tr \, \x \y$, show that $
\Y$ has a multivariate normal distribution on the space of $n \times
m$  matrices
\end{frame}

\end{document}

